{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57354a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61d5b4",
   "metadata": {},
   "source": [
    "# Getting data\n",
    "Only focusing on Danish data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9438813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import plotly.express as px\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02606c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"DDSC/nordic-embedding-training-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde079cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsdk = ds.filter(lambda sample: True if sample[\"language\"] == \"danish\" else False)\n",
    "texts = dsdk[\"train\"][\"query\"] + dsdk[\"train\"][\"positive\"] + dsdk[\"train\"][\"negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73132fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dsdk[\"train\"][\"positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d668b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[7912]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2f5bf",
   "metadata": {},
   "source": [
    "# Tokenizing dataset\n",
    "We want a small model. For static models, the size of the model scales with the number of tokens. To reduce the number of tokens we will therefore add a normalizer that lower-cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from plotly import express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f451d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_regex = regex.compile(r\"\\w+\")\n",
    "\n",
    "# Tokenize all texts\n",
    "tokens = []\n",
    "\n",
    "for text in tqdm(texts, desc=\"Tokenizing texts\"):\n",
    "    if text:\n",
    "        tokens.extend(tokenizer_regex.findall(text.lower()))\n",
    "\n",
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b801aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6011b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.DataFrame(token_counts.most_common(), columns=[\"word\", \"counts\"])\n",
    "counts_df = counts_df.reset_index().rename(columns={\"index\": \"rank\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df[\"prob\"] = counts_df[\"counts\"] / counts_df[\"counts\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea73bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_idx = list(range(1000)) + list(range(1000, counts_df.shape[0], 1000))\n",
    "len(vis_idx)\n",
    "px.line(counts_df.take(vis_idx), x=\"rank\", y=\"counts\", hover_data=[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f0108",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vertical_rows = 100\n",
    "fig = px.line(\n",
    "    counts_df.take(vis_idx), x=\"rank\", y=\"counts\", hover_data=[\"word\"], log_x=True\n",
    ")\n",
    "\n",
    "for idx, count_line in (\n",
    "    counts_df.groupby(\"counts\")\n",
    "    .first()\n",
    "    .sort_values(by=\"rank\", ascending=False)\n",
    "    .iloc[:n_vertical_rows]\n",
    "    .iterrows()\n",
    "):\n",
    "    fig.add_vline(x=count_line[\"rank\"])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee289fe2",
   "metadata": {},
   "source": [
    "From around index 260k, there is only a single count. \n",
    "From around index 190k, there is only two counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba08bc0",
   "metadata": {},
   "source": [
    "# Understanding weighting in model2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d554a7b4",
   "metadata": {},
   "source": [
    "The distillation in model2vec includes a post-processing step which requires the size of the final token space. This includes built-in tokens and the added vocabulary. To assess the weighting, we need to create the total token space first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf5d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from model2vec.tokenizer import (\n",
    "    clean_and_create_vocabulary,\n",
    "    replace_vocabulary,\n",
    "    turn_tokens_into_ids,\n",
    ")\n",
    "from model2vec.distill.inference import (\n",
    "    PCADimType,\n",
    "    create_embeddings,\n",
    "    post_process_embeddings,\n",
    ")\n",
    "from tokenizers import normalizers\n",
    "from typing import Optional, cast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"jealk/llm2vec-scandi-mntp-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e636c",
   "metadata": {},
   "source": [
    "We are adding the lower-case vocabulary, so we need to add a normaliser to the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of tokenizer\", type(tokenizer))\n",
    "print(\"Type of backend tokenizer\", type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ab859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current normalizer\n",
    "print(\"Current backend normalizer:\", tokenizer.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a9d9b",
   "metadata": {},
   "source": [
    "The current model does not have a normaliser, but we want to add one that ignores casing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lowercase normalization\n",
    "if tokenizer.backend_tokenizer.normalizer is not None:\n",
    "    # Keep existing normalizers and add lowercase\n",
    "    tokenizer.backend_tokenizer.normalizer = normalizers.Sequence(\n",
    "        [tokenizer.backend_tokenizer.normalizer, normalizers.Lowercase()]\n",
    "    )\n",
    "else:\n",
    "    # No existing normalizer, just add lowercase\n",
    "    tokenizer.backend_tokenizer.normalizer = normalizers.Lowercase()\n",
    "\n",
    "print(\"New normalizer:\", tokenizer.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01bd8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f93acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3fa6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba78e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dkmodel2vec.vocab import add_instruction_tokenizer\n",
    "\n",
    "t2 = add_instruction_tokenizer(tokenizer, instruction=DANISH_INSTRUCTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed82967",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.decode(t2.encode(\"fjaldf\"), add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc4f51",
   "metadata": {},
   "source": [
    "The prepend method allows us to add instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd475cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What preprocessing-related attributes exist?\n",
    "attrs = [\n",
    "    attr\n",
    "    for attr in dir(tokenizer)\n",
    "    if \"preprocess\" in attr.lower()\n",
    "    or \"prefix\" in attr.lower()\n",
    "    or \"template\" in attr.lower()\n",
    "]\n",
    "print(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b015bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New normalizer:\", tokenizer.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with mixed case text\n",
    "test_texts = [\"Hej Sverige!\", \"DETTE ER DANSK TEKST\", \"Mixed CaSe TeXt\"]\n",
    "\n",
    "print(\"Testing lowercase normalization:\")\n",
    "for text in test_texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"Input: '{text}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(\n",
    "        f\"Decoded: '{tokenizer.decode(tokenizer.encode(text), skip_special_tokens=False)}'\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15e0ef",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081db1dc",
   "metadata": {},
   "source": [
    "In the following we go through the different steps in the distillation script in model2vec/distilllation.py: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd1349",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_tokenizer = tokenizer.backend_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e0657",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = counts_df[\"word\"].tolist()  # vocabulary is sorted by popularity\n",
    "\n",
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbf37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens_before = len(vocabulary)\n",
    "\n",
    "print(f\"{n_tokens_before} individual words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc054d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_remove_regex = None\n",
    "all_tokens, backend_tokenizer_new_normalizer = clean_and_create_vocabulary(\n",
    "    tokenizer, vocabulary, token_remove_regex=token_remove_regex\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_tokenizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37475b26",
   "metadata": {},
   "source": [
    "The BPE tokenizer is a byte-pair encoder which allows tokenization of all strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_tokenizer_new_normalizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model2vec\n",
    "\n",
    "model2vec.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ec572",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens_after = len([token for token in all_tokens if not token.is_internal])\n",
    "print(f\"{n_tokens_after} external tokens (added from vocabulary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5156a1b6",
   "metadata": {},
   "source": [
    "There are still >600k tokens so this is waaay too big.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39239f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3fbbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[token for token in tokenizer.get_vocab() if \".\" == token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = cast(Optional[str], tokenizer.special_tokens_map.get(\"unk_token\"))\n",
    "pad_token = cast(Optional[str], tokenizer.special_tokens_map.get(\"pad_token\"))\n",
    "\n",
    "# Weird if to satsify mypy\n",
    "\n",
    "if unk_token is None:\n",
    "    unk_token = cast(\n",
    "        Optional[str], [token for token in tokenizer.get_vocab() if \"_\" == token][0]\n",
    "    )\n",
    "    print(\n",
    "        \"The unknown token is not set. Setting it to the '_' token. This is a workaround to allow encoding of more texts without error.\"\n",
    "    )\n",
    "if pad_token is None:\n",
    "    if unk_token is not None:\n",
    "        pad_token = unk_token\n",
    "        print(\n",
    "            \"The pad token is not set. Setting it to the unk token. This is a workaround for models that don't have a pad token.\"\n",
    "        )\n",
    "    else:\n",
    "        pad_token = unk_token or all_tokens[0].form\n",
    "        print(\n",
    "            \"The pad token is not set. Setting it to the first token in the vocabulary. This is a workaround for models that don't have a pad token.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca06ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"unk token: '{unk_token}', pad_token: '{pad_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f1a30",
   "metadata": {},
   "source": [
    "unk_token is None by default because I am looking at a BPE tokenizer which can tokenizer anything and therefore does not require an unk token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"_\" in tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432db0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model2vec.tokenizer.datamodels import Token\n",
    "\n",
    "unk_token_obj = Token(\n",
    "    form=\"[UNK]\", normalized_form=\"[UNK]\", is_subword=False, is_internal=False\n",
    ")\n",
    "all_tokens_with_unk_token = all_tokens + [unk_token_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a38db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the vocabulary in the tokenizer with the new vocabulary.\n",
    "backend_tokenizer_replaced_vocab = replace_vocabulary(\n",
    "    backend_tokenizer_new_normalizer,\n",
    "    all_tokens_with_unk_token,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=pad_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_tokenizer_replaced_vocab.model\n",
    "# backend_tokenizer_replaced_vocab.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ed632",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_tokenizer_replaced_vocab.encode(texts[12167])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9bb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_tokens = [token for token in all_tokens if token.is_internal == True]\n",
    "external_tokens = [token for token in all_tokens if token.is_internal == False]\n",
    "print(\n",
    "    f\"Internal tokens: {len(internal_tokens)}, External tokens: {len(external_tokens)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6383a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_tokens_with_unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens to IDs\n",
    "token_ids = turn_tokens_into_ids(all_tokens_with_unk_token, tokenizer, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids[300_000:300_050]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91dfc24",
   "metadata": {},
   "source": [
    "We need to check that the tokens_ids decode properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0636bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(token_ids[-2])  # last token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_with_unk_token[-2].form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fbce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.decode(token_ids[-2]) == tokenizer.decode(\n",
    "    tokenizer.encode(all_tokens_with_unk_token[-2].form), skip_special_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f848f",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07baa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac100b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dkmodel2vec.distillation import estimate_token_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff81fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_token_frequencies(\n",
    "    backend_tokenizer=backend_tokenizer_replaced_vocab,\n",
    "    corpus_texts=[\"list fjkl\", \"Hej \"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_tokenizer_replaced_vocab.encode(\n",
    "    'Analyser denleichtige metaforiske anvendelse af lyset i H.C. Andersens eventyr \"Den lille pige med svovlstikkerne\" og \"HyacintensSystematik_\".'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[7912]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5663d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[12167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_tokenizer_replaced_vocab.encode(\".\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de245fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_tokenizer_replaced_vocab.encode(texts[12167])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = estimate_token_frequencies(\n",
    "    backend_tokenizer=backend_tokenizer_replaced_vocab,\n",
    "    corpus_texts=texts,\n",
    "    batch_size=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26061265",
   "metadata": {},
   "source": [
    "We are creating a new tokenizer and accompanying embeddings.\n",
    "\n",
    "As long as we are guaranteed that the tokens in the new tokenizer has had created the correct embeddings then we are safe. I have added a new normalizer. That ought not to change anything. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d12f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d21f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda241b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dkmodel2vec.distillation import weigh_by_freq\n",
    "\n",
    "total = token_counts.total()\n",
    "weights = np.asarray([total / count_n for _, count_n in token_counts.most_common()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a80fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a221399",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights / np.abs(np.max(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_tokenizer_replaced_vocab.id_to_token(601)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca7a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts.total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecfaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted = [\n",
    "    (backend_tokenizer_replaced_vocab.id_to_token(id), id, count)\n",
    "    for id, count in token_counts.most_common()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdbd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_token = pd.DataFrame(counted, columns=[\"token\", \"id\", \"count\"]).sort_values(\n",
    "    by=\"count\", ascending=False\n",
    ")\n",
    "counts_token[\"rank\"] = counts_token.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only show these indices\n",
    "vis_idx = list(range(1000)) + list(range(1000, counts_token.shape[0], 1000))\n",
    "\n",
    "# number of vertical lines\n",
    "n_vertical_rows = 100\n",
    "fig = px.line(\n",
    "    counts_token.take(vis_idx),\n",
    "    x=\"rank\",\n",
    "    y=\"count\",\n",
    "    hover_data=[\"token\", \"id\", \"rank\"],\n",
    "    log_x=True,\n",
    ")\n",
    "\n",
    "for idx, count_line in (\n",
    "    counts_token.groupby(\"count\")\n",
    "    .first()\n",
    "    .sort_values(by=\"rank\", ascending=False)\n",
    "    .iloc[:n_vertical_rows]\n",
    "    .iterrows()\n",
    "):\n",
    "    fig.add_vline(x=count_line[\"rank\"])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e908e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_token_ids = [n[0] for n in token_counts.most_common()]\n",
    "seen_set = set(seen_token_ids)\n",
    "\n",
    "# Single list comprehension: ordering is seen ->unseen token\n",
    "sorted_tokens = [all_tokens_with_unk_token[token_id] for token_id in seen_token_ids] + [\n",
    "    token for i, token in enumerate(all_tokens_with_unk_token) if i not in seen_set\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a836aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tokens[-300_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a51f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_tokenizer_replaced_vocab.encode(\"Hej,  Jeg hedder anders\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea39eb",
   "metadata": {},
   "source": [
    "Looks good..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae993c",
   "metadata": {},
   "source": [
    "Model2vec uses the following approach to weigh the embedding of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420447e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(token_size: int, sif_coefficient: float):\n",
    "    \"\"\"Calculate weight for each token using Zipf's law and the SIF coefficient.\"\"\"\n",
    "    inv_rank = 1 / (np.arange(2, token_size + 2))\n",
    "    proba = inv_rank / np.sum(inv_rank)\n",
    "    weights = sif_coefficient / (sif_coefficient + proba)\n",
    "    return weights\n",
    "\n",
    "\n",
    "sif_coefficient = 1e-4\n",
    "\n",
    "counts_token[\"estimated_weight\"] = get_weights(\n",
    "    token_size=counts_token.shape[0], sif_coefficient=sif_coefficient\n",
    ")\n",
    "counts_token[\"estimated_prob\"] = 1 / counts_token[\"estimated_weight\"]\n",
    "counts_token[\"estimated_prob\"] = (\n",
    "    counts_token[\"estimated_prob\"] / counts_token[\"estimated_weight\"].sum()\n",
    ")\n",
    "\n",
    "counts_token[\"prob\"] = counts_token[\"count\"] / counts_token[\"count\"].sum()\n",
    "\n",
    "counts_token[\"prob/estimated_prob\"] = (\n",
    "    counts_token[\"prob\"] / counts_token[\"estimated_prob\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec867c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    counts_token.iloc[vis_idx],\n",
    "    x=\"rank\",\n",
    "    y=[\"prob\", \"estimated_prob\"],\n",
    "    hover_data=[\"token\", \"count\"],\n",
    "    log_x=True,\n",
    "    log_y=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2e6d1",
   "metadata": {},
   "source": [
    "Hmmm.. For this token_size it look like it underestimates the probability of frequent words and underestimates the frequency of rare words. Let's instead just use the weights from the actual dataset. This means we set the SIF coefficient to zero and implement our own post-processing and distillation script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff81025",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkmodel2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
